+++
date = '2025-09-15T15:45:55+08:00'
lastmod = '2025-09-16T15:45:55+08:00'
title = 'Math101：从向量到SVD矩阵分解'
description = ""
author = "Lloyd Sun"
tags = []
categories = []
ShowToc = true
ShowReadingTime = true
ShowWordCount = false
ShowPostNavLinks = true

TocOpen = false
searchHidden = false
ShowShareButtons = false
ShowBreadCrumbs = false
draft = false

+++

# 从向量到SVD矩阵分解

本文以向量为对象，从线性变换的几何意义出发，以矩阵分解中的SVD方法为终点，总结线性代数的核心理念。

## 01 向量及其线性组合(Linear Combination)

向量是线性代数的基本操作对象。从物理角度来说，向量是有大小、有方向的一个箭头；从计算机角度，向量是列表中的一系列数字；而我们更熟悉的形式，是一个起点在原点、终点在坐标系中某个位置的一条线段，通常用终点位置的坐标来表示这个向量。而线性代数，就是研究向量之间的线性组合和线性变换的，具体来说，线性代数只关注向量加法与向量的缩放，即$\vec{V}+\vec{W}$和$a\vec{V}$这两种操作，而这两种操作的组合，就被称为线性组合，即$a\vec{V}+b\vec{W}$。

对于一个具体的向量来说（习惯上，单个向量用列向量来表示），都可以拆解为某种基(Basis)向量的线性组合。以2D空间中的向量$\begin{bmatrix}
a \\\\
 b
\end{bmatrix}$（注意，习惯上会写成列向量），可以表示为$a\hat{i}+b\hat{j}$，其中$\hat{i}与\hat{j}$分别是x和y轴方向的单位向量，其中的a和b看作是对单位向量$\hat{i}与\hat{j}$缩放系数。从运动的视角来看，这个向量整体，相当于是先沿着$\hat{i}$方向(即x轴)走了距离a，再沿着$\hat{j}$方向(即y轴)走了距离b，a、b的正负代表具体方向、大小代表具体距离。总结来说，任一个向量都可以看作基向量的线性组合。

以上内容即为线性代数的基本出发点，在这个基础上，还有几个常用的概念。

- 第一，对于任意的几个向量来说，其所有可能的线性组合被称为其张成（span）的空间，举例来说，对于$\vec{V}和\vec{W}$，则$a\vec{V}+b\vec{W}$的则为这两个向量的张成(Span)；
- 第二，如果新增的一个向量已经在其他向量的张成空间里了，也就是新增的向量并没有增加关于空间的新的信息，则这个新增的向量是“多余的”。具体来说，这个多余的向量可以表示为其他向量的线性组合，即$\vec{new}=a\vec{V}+b\vec{W}$，所以我们会说这几个向量是线性相关的；反之会称为线性无关(Linear Independant)；
- 第三，一个空间的基向量不一定非要像$\hat{i}与\hat{j}$这么标准(长度都为1，并且是垂直/正交的)，实际上基向量只要求是线性无关，并且他们的能够张成整个空间即可。

以上便是线性代数的基本出发点，其中最终要的观点是，任何一个向量都可以看作是基向量的线性组合，即$ \begin{bmatrix}
a \\\\
 b
\end{bmatrix} = a\hat{i}+b\hat{j}$。这对理解线性变换是至关重要的。

## 02 线性变换(Linear Transformation)

直观理解线性代数的关键是理解线性变换，具体来说，指的是矩阵与向量乘法的几何意义，或更具体地说，矩阵本身的作用是线性变换。基于上一节对向量的理解，我们已经知道，任何一个向量都可以表示成基向量的线性组合（或理解成在基向量方向上的运动组合）。即
$$
\begin{bmatrix}
x \\\\
 y
\end{bmatrix}
= x\hat{i}+y\hat{j} = x\begin{bmatrix}1 \\\\ 0\end{bmatrix} + y\begin{bmatrix}0 \\\\ 1\end{bmatrix}
$$
矩阵与向量乘法可以做如下的表示：
$$
\mathbf{A}\vec{X}=
\begin{bmatrix}
 a & b \\\\
 c & d
\end{bmatrix}
\begin{bmatrix}
 x \\\\
 y
\end{bmatrix}
= x\begin{bmatrix}a \\\\ c\end{bmatrix} + y\begin{bmatrix}b \\\\ d\end{bmatrix}
$$
这里的核心就是理解矩阵A从几何意义上看的实质作用是什么：矩阵A实质上做了线性变换，将原本的基向量（$\begin{bmatrix}1 \\\\ 0\end{bmatrix}$ 与 $\begin{bmatrix}0 \\\\ 1\end{bmatrix}$变成了$\begin{bmatrix}a \\\\ c\end{bmatrix}$ 与 $\begin{bmatrix}b \\\\ d\end{bmatrix}$。值得注意的细节是，代表线性变换的矩阵A，是放在左侧相乘的$\mathbf{A}\vec{X}$，例如对于矩阵乘法$\mathbf{B}\mathbf{A}\vec{X}$来说，可以理解为先把$\vec{X}$(的基向量)进行线性变换A，再进行线性变换B。

以上便是理解线性代数几何含义的最核心内容，在此基础之上，可以补充两个重要的信息：

1. 矩阵实际上等同于线性变换(Linear Transformation)，这里的变换(Transformation)与函数、算子等概念基本是等同的，只是更强调运动的部分。
2. 从几何意义上判断是否是线性变换也比较简单，即1）原本空间中的所有直线经变换后还是直线，更直观地是通过原本平行线是否扔平行并且平均分布来判读；2）原点保持不变。

上述的线性变换描述的矩阵A是正方形的(m*m矩阵)，且我们使用的是2维空间中的例子。对于更高维度的转换比较直观，对于非方阵的情况需要额外的解释。我们还用之前差不多的案例，只不过现在的A是一个$3 * 2$的矩阵。
$$
\mathbf{A}\vec{X}=\begin{bmatrix}
 a & b \\\\
 c & d \\\\
 e & f
\end{bmatrix}
\begin{bmatrix}
 x \\\\
 y
\end{bmatrix}
= x\begin{bmatrix}a \\\\ c \\\\ e\end{bmatrix} + y\begin{bmatrix}b \\\\ d \\\\ f\end{bmatrix}
$$

我们采用与之前相同的视角来看待这个$3 * 2$的矩阵A，可以自然推导出，A实质上是将原本的基向量$\hat{i}\,\text{与}\,\hat{j}$（$\begin{bmatrix}1 \\\\ 0\end{bmatrix}$ 与 $\begin{bmatrix}0 \\\\ 1\end{bmatrix}$）变成了 $\begin{bmatrix}a \\\\ c \\\\ e\end{bmatrix}$ 与 $\begin{bmatrix}b \\\\ d \\\\ f\end{bmatrix}$，也就是A将基向量从2维空间转换到了3维空间。所以，非正方形的矩阵（Non-Square Matrix）在方阵基础之上，还进行了空间维度的转换。距离来说，如果另一个$1 * 2$的矩阵B与向量$\vec{X}$相乘，那么原味2维空间的基向量就被转换到了1维空间。

## 03 行列式(Determinant)

行列式(Determinant)有非常直观且明确的几何意义。通过之前的讨论，我们已经知道，矩阵的几何意义是线性变换，例如对于一个矩阵 $\begin{bmatrix}2 & 0 \\\\ 0 & 3\end{bmatrix}$，它实质上相当与将 $\hat{i}\,\text{与}\,\hat{j}$ 转换到了 $\begin{bmatrix}2 \\\\ 0\end{bmatrix}$ 与 $\begin{bmatrix}0 \\\\ 3\end{bmatrix}$，那么原本基向量(单位向量)构成的矩形面积是1，新的基向量构成的矩形面积是6，那么这个矩阵的行列式就是6。

所以，行列式的几何含义是对原本“单位空间”的缩放比例。对于上述的2维空间，这个所谓的“单位空间”是指面积；对于1维空间，就是单位长度；对于3维空间，就是单位体积。

一个需要补充的细节是，行列式的这个缩放比例是可以有负值的，其负值的含义是是否对“单位空间”进行了翻转（当然这在高维空间不太好理解）。对于1维空间，就是方向是否翻转了；对于二维空间，正值代表变换后的$\hat{i}\&\hat{j}$，仍然保持$\hat{i}在\hat{j}$的右侧（想象一下默认情况下，x与y轴之间的位置关系）；对于三维空间，正值符合右手定则、负值符合左手定则。

基于行列式的几何含义，我们可以进一步进行推理。当矩阵A的行列式$\det{A}\neq0$时，A对原本基向量(或空间)的线性变换是可逆的，也就是可以通过另一个矩阵B，将A产生的线性变换给“翻转”过来。这个矩阵B被称为A的逆矩阵(Inverse Matrix)，一般用$A^{-1}$来表示，当然可以很直观地得到，$A*A^{-1}=I$，其中$I$是指单位矩阵，有着如下的表现形式：$\begin{bmatrix}
 1 & 0 \\\\
 0 & 1
\end{bmatrix}$（方阵，对角元素为1，其余为0）。

当矩阵A的行列式$\det{A}=0$时，其几何意义是原本的“单位空间“变成了0，这实际上意味着某些“维度“被压缩到原点了。举例来说，在经过$A=\begin{bmatrix} 1 & 1 \\\\
 1 & 1\end{bmatrix}$的变换，原本的基向量$\hat{i}与\hat{j}$都被移动到了(1, 1)的位置，即被变换的向量X从2维空间到了1维空间中(新的基向量的张成是一条直线)。用更“线性代数”的语言来描述，原本空间的秩(Rank)是2，转换后的秩是1，即空间的秩下降了。从空间的角度来看，A的所有列向量的张成组成了列空间（column space）；在这个例子中我们可以看出，A的两个列向量是线性相关（即一个列向量可以通过另一个列向量的缩放得到）的，这实际上代表有一个列向量给出了“冗余的信息”。一般来说，在列向量存在冗余信息的情况下$\det{A}$一定为零，意味着一定有一些空间被压缩到了原点，而这些被压缩到原点的向量空间称为零空间(Null Space)，也叫做矩阵核(Kernel)。

额外简要说明的是，矩阵的四个核心子空间别分别列空间、行空间(Row Space)、零空间和左零空间(Left Null Space)，本文只对1、3两个空间进行了说明。有兴趣的读者可以进一步了解其他两个子空间。

## 04 点积与交叉积(Dot Product & Cross Product)

点积和交叉积是向量运算中的两个基本操作，它们都有着清晰的几何意义。

### 4.1 点积的几何意义

点积（也称为内积）的计算非常简单，对于两个向量$\vec{v} = \begin{bmatrix} v_1 \\\\ v_2 \end{bmatrix} $和$\vec{w} = \begin{bmatrix} w_1 \\\\ w_2 \end{bmatrix} $，其点积为：
$$
\vec{v} \cdot \vec{w} = v_1 w_1 + v_2 w_2
$$
从几何意义上看，点积等于一个向量在另一个向量方向上的投影长度，再乘以另一个向量的长度。更具体地说：
$$
\vec{v} \cdot \vec{w} = |\vec{v}| \cdot |\vec{w}| \cdot \cos\theta
$$
其中$\theta $是两个向量之间的夹角。这个公式揭示了点积的核心含义：它衡量了两个向量的"相似程度"——当两个向量方向相同时点积最大，垂直时为零，方向相反时为负。

从线性变换的视角来看，点积还有另一种理解方式。计算$\vec{v} \cdot \vec{w} $实际上等价于将$\vec{v} $转置后与$\vec{w} $进行矩阵乘法：
$$
\vec{v} \cdot \vec{w} = \vec{v}^T \vec{w} = \begin{bmatrix} v_1 & v_2 \end{bmatrix} \begin{bmatrix} w_1 \\\\ w_2 \end{bmatrix}
$$
这意味着点积本质上是将高维空间投影到一维空间的线性变换。

### 4.2 交叉积的几何意义

交叉积是三维空间特有的运算。对于两个三维向量$\vec{v} = \begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \end{bmatrix} $和$\vec{w} = \begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \end{bmatrix} $，其交叉积为：
$$
\vec{v} \times \vec{w} = \begin{bmatrix} v_2w_3 - v_3w_2 \\\\ v_3w_1 - v_1w_3 \\\\ v_1w_2 - v_2w_1 \end{bmatrix}
$$
上述公式很难记忆，而他实际表达的意思与几何特性如下：

1. **方向**：交叉积垂直于$\vec{v} $和$\vec{w} $所构成的平面，遵循右手定则（右手四指从$\vec{v} $转向$\vec{w} $，大拇指指向交叉积的方向）。
2. **大小**：交叉积等于$\vec{v} $和$\vec{w} $围成的平行四边形的面积

## 05 EVD与SVD

矩阵分解是线性代数中最重要的工具之一，它的核心思想是：能否为给定的线性变换找到一个特殊的坐标系，使得复杂的变换变得简单？

### 5.1 特征值与特征向量

想象你有一个线性变换A，它会改变大多数向量的方向和长度。但总有一些特殊的方向，A只是将这些方向上的向量进行了缩放，而没有改变方向。这些特殊的方向就是特征向量。

举个具体例子：矩阵$A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}$作用在向量$\vec{v} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$上时： $$A\vec{v} = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 3 \\ 0 \end{bmatrix} = 3\vec{v}$$

向量$\vec{v} $的方向没有改变，只是被放大了3倍！这就是特征向量，对应的放大倍数3就是特征值。

回顾第二节的内容，我们知道任何向量都可以表示为基向量的线性组合。通常我们使用标准基$\hat{i}$和$\hat{j}$，但实际上可以选择任何一组线性无关的向量作为基。变基的核心思想是： 同一个向量在不同基下的坐标表示会不同，但向量本身不变。如果有新基向量组成的矩阵$P $，那么从标准基到新基的变换公式是：新坐标 = $P^{-1} \times $ 原向量。

现在考虑一个重要问题：如果我们把坐标系的基向量选择为A的特征向量，会发生什么？假设A有两个特征向量$\vec{v}_1$和$\vec{v}_2$，如果用它们作为新的基，那么在这个坐标系下，A对每个基向量的作用变得极其简单：只是独立的缩放，没有任何"混合"效应。换句话说，A在特征基下变成了对角矩阵。

### 5.2 特征值分解EVD

基于上面的观察，特征值分解(EVD)的思想就很自然了：既然在特征基下矩阵A变成对角矩阵，那我们就利用这个特点来简化计算。

EVD将复杂的线性变换A分解为三个简单步骤：先将输入从标准基转换到特征基，在特征基下进行简单的对角操作，最后将结果从特征基转换回标准基。这就是经典的$A = Q\Lambda Q^{-1}$分解形式。

为什么这样做有用？对角矩阵的计算非常简单。比如要计算$A^{100}$，在标准基下很复杂，但在特征基下只需要将每个特征值分别求100次方。更重要的是，EVD揭示了线性变换的"本质结构"——它告诉我们变换在哪些方向上作用最强，哪些方向作用最弱。

但EVD只适用于方形矩阵，而且要求有足够多的线性无关特征向量。对于非方形矩阵怎么办？

### 5.3 奇异值分解SVD

考虑一个$3 \times 2$的矩阵A，它将2维输入映射到3维输出。这里我们面临新问题：输入空间是2维的，输出空间是3维的，这两个空间完全不同，不能简单地用EVD的方法。

奇异值分解(SVD)提出了巧妙的解决方案：既然输入空间和输出空间不同，那就分别为它们寻找最优的基。关键问题是什么叫"最优"？对于输入空间，我们要找到使得A作用效果最强的方向；对于输出空间，我们要找到A最倾向于输出的方向。

如何找到这些最优方向？对于输入空间，我们要解决这样一个优化问题：在所有单位向量中，哪个方向能让$|A\vec{x}|$最大？这个问题的解恰好是$A^TA$的特征向量。类似地，$AA^T$的特征向量给出了输出空间的最优方向。

这里有个重要的数学事实：$A^TA$和$AA^T$都是 对称矩阵（即矩阵等于自己的转置），而对称矩阵有一个优美的性质——它们的特征向量天然正交，可以构成标准正交基。这就保证了我们找到的"最优方向"不仅是最优的，而且相互垂直，形成了完美的坐标系。正是因为这个性质，SVD中的$U$和$V$都是正交矩阵，

现在SVD的三步就很自然了，它遵循与EVD完全相同的逻辑：变基→变换→变基。第一步$V^T$将输入从标准基转换到右奇异向量基，第二步$\Sigma$在最优基下进行缩放并完成维度转换，第三步$U$将结果从左奇异向量基转换到标准基。

关键差异在于，SVD的两次变基操作处理的是不同的空间——输入空间和输出空间各有自己的最优基。至于为什么V需要转置，答案很简单：这就是标准的变基操作形式。在SVD中，$V$是正交矩阵，因此$V^{-1} = V^T$，所以$V^T$就是从标准基到右奇异向量基的变基操作。

### 5.4 统一的视角

虽然EVD和SVD处理不同的问题，但它们的哲学是一致的：通过选择合适的坐标系，让复杂的线性变换变得简单。

在建立了几何理解之后，我们可以给出它们的标准数学形式：
$$
特征值分解(EVD)：A = Q\Lambda Q^{-1} \\
奇异值分解(SVD)：A = U\Sigma V^T
$$
对于EVD，计算步骤相对直观：首先求解特征方程$\det(A - \lambda I) = 0$得到特征值$\lambda$，然后对每个特征值求解$(A - \lambda I)\vec{v} = 0$得到对应的特征向量$\vec{v} $。例如，对于前面提到的矩阵$A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}$，特征方程为$\det\begin{bmatrix} 3-\lambda & 1 \\ 0 & 2-\lambda \end{bmatrix} = (3-\lambda)(2-\lambda) = 0$，得到特征值$\lambda_1 = 3, \lambda_2 = 2$。

对于SVD，计算过程是先分别计算$A^TA$和$AA^T$的特征值分解，得到右奇异向量矩阵$V$和左奇异向量矩阵$U$，奇异值则是$A^TA$特征值的平方根。

这两个公式背后都遵循相同的三步逻辑：

| 步骤       | EVD                     | SVD                     |
| ---------- | ----------------------- | ----------------------- |
| **第一步** | $Q^{-1}$：标准基→特征基 | $V^T$：标准基→右奇异基  |
| **第二步** | $\Lambda$：特征基下缩放 | $\Sigma$：缩放+维度变换 |
| **第三步** | $Q$：特征基→标准基      | $U$：左奇异基→标准基    |

核心区别在于：EVD在同一空间内寻找最简单的表示，而SVD为不同空间分别寻找最优的表示。

这种"寻找最优坐标系"的思想在实际应用中极其有用。无论是数据压缩、降维、去噪，还是特征提取，核心都是找到数据的"最重要方向"。通过理解变基的几何意义，我们不仅掌握了计算方法，更重要的是理解了为什么这些方法有效——它们都在为复杂的线性变换寻找最自然、最简洁的表达方式。

## 补充

以上便是线性代数的全部核心内容：我们从线性变换的视角出发，讨论了线性代数中关键概念的几何意义，同时并没有过多地介绍如何去计算，因此这通常可以借助计算机来完成。对公式与计算方法的熟悉当然有助于考试和对数字的敏感度，但这无关线性代数的本质。

从“线性变换“来理解线性代数本质的灵感源自于3B1B(3Blue1Brown, Grant Sanderson)的Essence of Linear Algebra[系列视频](https://www.bilibili.com/video/BV1Ys411k7yQ/?spm_id_from=333.337.search-card.all.click&vd_source=11cbe4e223f3ef3e00cac82a0cb79098)与[博客文章](https://www.3blue1brown.com/lessons/vectors#title)，本文实际上也是在完整观看了这个系列视频与文章之后的总结。对于希望能够体验更多可视化内容的读者，非常推荐这个系列，因为这也许是目前市面上最好的线性代数教程了。